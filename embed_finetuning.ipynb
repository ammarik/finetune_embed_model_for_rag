{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13def339-b539-4058-8e37-b6487084c39c",
   "metadata": {},
   "source": [
    "# Fine-tune Embedding models for Retrieval Augmented Generation (RAG)\n",
    "* Based on:\n",
    "    * [Phil Schmid - Fine-tune Embedding models for Retrieval Augmented Generation (RAG)](https://www.philschmid.de/fine-tune-embedding-model-for-rag?fbclid=IwY2xjawHfd8xleHRuA2FlbQIxMAABHajMHJeyzUk_IKqC3lS1-eZ7cejyi96lN1pJEqk_UmMGQhnWSspl53eW6w_aem_IygaSnTlA8yKtG_yiCMBlQ)\n",
    "    * [Phil Schmid's github](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-embedding-model-for-rag.ipynb)\n",
    "\n",
    "* We'll Fine-tune an embedding model for a financial RAG applications using a synthetic dataset from the 2023_10 NVIDIA SEC Filing.\n",
    "* We'll also leverage Matryoshka Representation Learning to boost efficiency.\n",
    "\n",
    "In this notebook, we are going to:\n",
    "1. Create & Prepare embedding dataset\n",
    "2. Create baseline and evaluate pretrained model\n",
    "3. Define loss function with Matryoshka Representation\n",
    "4. Fine-tune embedding model with SentenceTransformersTrainer\n",
    "5. Evaluate fine-tuned model against baseline\n",
    "\n",
    "\n",
    "### What are theðŸª†Matryoshka Embeddings?\n",
    "\n",
    "Matryoshka Representation Learning (MRL) is a technique designed to create embeddings that can be truncated to various dimensions without significant loss of performance. This approach frontloads important information into earlier dimensions of the embedding, allowing for efficient storage and processing while maintaining high accuracy in downstream tasks such as retrieval, classification, and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fcad61-99aa-440e-ba80-82c78ece0af0",
   "metadata": {},
   "source": [
    "## Imports & settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8a688-40d5-4cc3-8082-562575bf7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import torch\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from huggingface_hub import login\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformerModelCardData,\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments)\n",
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator)\n",
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5ef67-8f75-4a22-b0e5-8a6a70788fa5",
   "metadata": {},
   "source": [
    "## Login to HF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f7a49-4e76-4806-8cf8-ea1b528f28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=getpass.getpass(prompt='Insert your HF token.'), add_to_git_credential=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857fa766-4bcc-4287-896f-d42ae7b92870",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "We are going to use philschmid/finanical-rag-embedding-dataset, which includes 7,000 positive text pairs of questions and corresponding context from the 2023_10 NVIDIA SEC Filing.\n",
    "\n",
    "The dataset has the following format:\n",
    "```json\n",
    "{\"question\": \"<question>\", \"context\": \"<relevant context to answer>\"}\n",
    "{\"question\": \"<question>\", \"context\": \"<relevant context to answer>\"}\n",
    "{\"question\": \"<question>\", \"context\": \"<relevant context to answer>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db3322-a40c-4ac7-8ef8-205bb2fb4f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philschmid/finanical-rag-embedding-dataset\", split=\"train\")\n",
    " \n",
    "# rename columns (to match what sentence-transforemrs expects)\n",
    "dataset = dataset.rename_column(\"question\", \"anchor\")\n",
    "dataset = dataset.rename_column(\"context\", \"positive\")\n",
    " \n",
    "# Add an id column to the dataset\n",
    "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    " \n",
    "# split dataset into a 10% test set\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    " \n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c8bae-702e-40d3-942c-804de02208c8",
   "metadata": {},
   "source": [
    "## Create baseline and evaluate pretrained model\n",
    "We will use the [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) model as our starting point. BAAI/bge-base-en-v1.5 is one of the strongest open embedding models for it size, with only 109M parameters and a hidden dimension of 768 it achieves 63.55 on the MTEB Leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b93a8-efa4-40d6-8c83-570eb9525e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"BAAI/bge-base-en-v1.5\"  # Hugging Face model ID\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64] # Important: large to small\n",
    " \n",
    "# Load a model\n",
    "model = SentenceTransformer(\n",
    "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    " \n",
    "# load test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    " \n",
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(\n",
    "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
    ")  # Our corpus (cid => document)\n",
    "queries = dict(\n",
    "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
    ")  # Our queries (qid => question)\n",
    " \n",
    "# Create a mapping of relevant document (1 in our case) for each query\n",
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for q_id in queries:\n",
    "    relevant_docs[q_id] = [q_id]\n",
    " \n",
    " \n",
    "matryoshka_evaluators = []\n",
    "# Iterate over the different dimensions\n",
    "for dim in matryoshka_dimensions:\n",
    "    ir_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=f\"dim_{dim}\",\n",
    "        truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n",
    "        score_functions={\"cosine\": cos_sim},\n",
    "    )\n",
    "    matryoshka_evaluators.append(ir_evaluator)\n",
    " \n",
    "# Create a sequential evaluator\n",
    "evaluator = SequentialEvaluator(matryoshka_evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17442aa5-c38e-477d-9cd3-4c065a0531d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = evaluator(model)\n",
    " \n",
    "# # COMMENT IN for full results\n",
    "# print(results)\n",
    " \n",
    "# Print the main score\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    print\n",
    "    print(f\"{key}: {results[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70bb637-94fc-40cc-9f4b-17b86b1b40c0",
   "metadata": {},
   "source": [
    "Latest results:\n",
    "|---|\n",
    "|dim_768_cosine_ndcg@10: 0.744207805513057|\n",
    "|dim_512_cosine_ndcg@10: 0.7374662163561584|\n",
    "|dim_256_cosine_ndcg@10: 0.7299773584859578|\n",
    "|dim_128_cosine_ndcg@10: 0.6960945771475592|\n",
    "|dim_64_cosine_ndcg@10: 0.6351348491423877|\n",
    "\n",
    "Now, let's see if we can improve this score by fine-tuning the model on our specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e8e4a-3c5d-40f4-bc9d-5497c1d76967",
   "metadata": {},
   "source": [
    "## Define loss function with Matryoshka Representation\n",
    "* For Positive Text pairs we can use the `MultipleNegativesRankingLoss` in combination with the `MatryoshkaLoss`.\n",
    "* The `MultipleNegativesRankingLoss` is a great loss function if you only have positive pairs as it adds in batch negative samples to the loss function to have per sample n-1 negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cc200-60f0-4566-a25a-8faf436fddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model ID: https://huggingface.co/BAAI/bge-base-en-v1.5\n",
    "model_id = \"BAAI/bge-base-en-v1.5\"\n",
    " \n",
    "# load model with SDPA for using Flash Attention 2\n",
    "model = SentenceTransformer(\n",
    "    model_id,\n",
    "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"BGE base Financial Matryoshka\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76ccc1-a339-448d-b15b-c5826cdec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matryoshka_dimensions = [768, 512, 256, 128, 64]  # Important: large to small\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55d235-b25d-43cb-b7fe-c7586e16a0d8",
   "metadata": {},
   "source": [
    "##  Fine-tune embedding mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953893b-62a7-49d1-a5ad-ddf0ff50912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset again\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    " \n",
    "# define training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"bge-base-financial-matryoshka\", # output directory and hugging face model ID\n",
    "    num_train_epochs=4,                         # number of epochs\n",
    "    per_device_train_batch_size=8,              # train batch size\n",
    "    gradient_accumulation_steps=8,              # for a global batch size of 512\n",
    "    per_device_eval_batch_size=4,               # evaluation batch size\n",
    "    warmup_ratio=0.1,                           # warmup ratio\n",
    "    learning_rate=2e-5,                         # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",                 # use constant learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    tf32=False,                                 # use tf32 precision\n",
    "    bf16=True,                                  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"epoch\",                      # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",                      # save after each epoch\n",
    "    logging_steps=10,                           # log every 10 steps\n",
    "    save_total_limit=3,                         # save only the last 3 models\n",
    "    load_best_model_at_end=True,                # load the best model when training ends\n",
    "    metric_for_best_model=\"eval_dim_128_cosine_ndcg@10\",  # Optimizing for the best ndcg@10 score for the 128 dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ba210-3225-4dba-9ffe-59c1c063f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model, # bg-base-en-v1\n",
    "    args=args,  # training arguments\n",
    "    train_dataset=train_dataset.select_columns(\n",
    "        [\"positive\", \"anchor\"]\n",
    "    ),  # training dataset\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe907c-c58d-4585-b138-4506ac5a04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    " \n",
    "# save the best model\n",
    "trainer.save_model()\n",
    " \n",
    "# push model to hub\n",
    "trainer.model.push_to_hub(\"bge-base-financial-matryoshka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe9bd3-fa75-415f-8f94-339766f5d1cf",
   "metadata": {},
   "source": [
    "##  Evaluate fine-tuned model against baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacaea33-3d47-48b3-9dbe-19c506988122",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = SentenceTransformer(\n",
    "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "# Evaluate the model\n",
    "results = evaluator(fine_tuned_model)\n",
    " \n",
    "# # COMMENT IN for full results\n",
    "# print(results)\n",
    " \n",
    "# Print the main score\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    print(f\"{key}: {results[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6038af75-d89b-48b1-9e34-7b8c777bfe4e",
   "metadata": {},
   "source": [
    "Latest results:\n",
    "\n",
    "|Original|Fine-tuned|\n",
    "|---|---|\n",
    "|dim_768_cosine_ndcg@10: 0.744207805513057|dim_768_cosine_ndcg@10: 0.7918768662814302|\n",
    "|dim_512_cosine_ndcg@10: 0.7374662163561584|dim_512_cosine_ndcg@10: 0.7941777731436057|\n",
    "|dim_256_cosine_ndcg@10: 0.7299773584859578|dim_256_cosine_ndcg@10: 0.7887004228679669|\n",
    "|dim_128_cosine_ndcg@10: 0.6960945771475592|dim_128_cosine_ndcg@10: 0.7753739833293516|\n",
    "|dim_64_cosine_ndcg@10: 0.6351348491423877|dim_64_cosine_ndcg@10: 0.7482035790215306|\n",
    "\n",
    "The fine-tuned model outperforms the original model with embedding size 768 even when using embedding size 64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481f4fc-f5ee-472d-b23d-3c8d391e2666",
   "metadata": {},
   "source": [
    "# Some other examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5a280-9613-43e8-84f6-4e7833d7fef0",
   "metadata": {},
   "source": [
    "Here is an example that encodes sentences and then computes the distance between them for doing semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a4457-f5aa-46e1-82f1-2b263204da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f57c8-562d-47b2-abbf-6c8dd4d07731",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district'])\n",
    "\n",
    "print(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e3cfe-4d16-4d6c-8d6e-980a0493abf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
